

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>2.9. scikit-learn: machine learning in Python &mdash; Scipy lecture notes</title>
    <link rel="stylesheet" href="../../_static/default.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../',
        VERSION:     '2011',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <link rel="top" title="Scipy lecture notes" href="../../index.html" />
    <link rel="up" title="2. Advanced topics" href="../index.html" />
    <link rel="prev" title="2.8. Sympy : Symbolic Mathematics in Python" href="../sympy.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../sympy.html" title="2.8. Sympy : Symbolic Mathematics in Python"
             accesskey="P">previous</a></li>
        <li><a href="../../index.html">Scipy lecture notes</a> &raquo;</li>
          <li><a href="../index.html" accesskey="U">2. Advanced topics</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
          <div class="body">
            
  <div class="section" id="scikit-learn-machine-learning-in-python">
<h1>2.9. scikit-learn: machine learning in Python<a class="headerlink" href="#scikit-learn-machine-learning-in-python" title="Permalink to this headline">¶</a></h1>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">author:</th><td class="field-body">Fabian Pedregosa</td>
</tr>
</tbody>
</table>
<p>Machine learning is a rapidly-growing field with several machine
learning frameworks available for Python:</p>
<p class="centered">
<strong><a class="reference internal image-reference" href="../../_images/mdp.png"><img alt="mdp" src="../../_images/mdp.png" style="width: 145.6px; height: 70.0px;" /></a>
  <a class="reference internal image-reference" href="../../_images/mlpy_logo.png"><img alt="mlpy" src="../../_images/mlpy_logo.png" style="width: 126.7px; height: 73.5px;" /></a>
  <a class="reference internal image-reference" href="../../_images/orange-logo-w.png"><img alt="orange" src="../../_images/orange-logo-w.png" style="width: 137.9px; height: 59.5px;" /></a>
 <a class="reference internal image-reference" href="../../_images/scikit-learn-logo.png"><img alt="skl" src="../../_images/scikit-learn-logo.png" style="width: 226.4px; height: 80.8px;" /></a>
</strong></p><div class="topic">
<p class="topic-title first">Prerequisites</p>
<ul class="simple">
<li>Numpy, Scipy</li>
<li>IPython</li>
<li>matplotlib</li>
<li>scikit-learn (<a class="reference external" href="http://scikit-learn.sourceforge.net">http://scikit-learn.sourceforge.net</a>)</li>
</ul>
</div>
<div class="contents local topic" id="chapters-contents">
<p class="topic-title first">Chapters contents</p>
<ul class="simple">
<li><a class="reference external" href="#loading-an-example-dataset" id="id1">Loading an example dataset</a><ul>
<li><a class="reference external" href="#learning-and-predicting" id="id2">Learning and Predicting</a></li>
</ul>
</li>
<li><a class="reference external" href="#supervised-learning" id="id3">Supervised learning</a><ul>
<li><a class="reference external" href="#k-nearest-neighbors-classifier" id="id4">k-Nearest neighbors classifier</a></li>
<li><a class="reference external" href="#support-vector-machines-svms-for-classification" id="id5">Support vector machines (SVMs) for classification</a></li>
</ul>
</li>
<li><a class="reference external" href="#clustering-grouping-observations-together" id="id6">Clustering: grouping observations together</a><ul>
<li><a class="reference external" href="#k-means-clustering" id="id7">K-means clustering</a></li>
<li><a class="reference external" href="#dimension-reduction-with-principal-component-analysis" id="id8">Dimension Reduction with Principal Component Analysis</a></li>
</ul>
</li>
<li><a class="reference external" href="#putting-it-all-together-face-recognition-with-support-vector-machines" id="id9">Putting it all together : face recognition with Support Vector Machines</a></li>
</ul>
</div>
<div class="section" id="loading-an-example-dataset">
<h2><a class="toc-backref" href="#id1">2.9.1. Loading an example dataset</a><a class="headerlink" href="#loading-an-example-dataset" title="Permalink to this headline">¶</a></h2>
<div style='float: right; margin: 35px;'><img align="right" alt="Photo of Iris Virginia" class="align-right" src="../../_images/Virginia_Iris.png" />
</div><p>First we will load some data to play with. The data we will use is a
very simple flower database known as the Iris dataset.</p>
<p>We have 150 observations of the iris flower specifying some of its
characteristics: sepal length, sepal width, petal length and petal
width together with its subtype: Iris Setosa, Iris Versicolour, Iris
Virginica.</p>
<p>To load the dataset into a Python object:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scikits.learn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
</pre></div>
</div>
<p>This data is stored in the <tt class="docutils literal"><span class="pre">.data</span></tt> member, which
is a <tt class="docutils literal"><span class="pre">(n_samples,</span> <span class="pre">n_features)</span></tt> array.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(150, 4)</span>
</pre></div>
</div>
<p>It is made of 150 observations of irises, each described by the 4
features mentioned earlier.</p>
<p>The information about the class of each observation is stored in the
target attribute of the dataset. This is an integer 1D array of length
<tt class="docutils literal"><span class="pre">n_samples</span></tt>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(150,)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="go">[0, 1, 2]</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title first">An example of reshaping data: the digits dataset</p>
<a class="reference internal image-reference" href="../../_images/digits_first_image.png"><img align="right" alt="../../_images/digits_first_image.png" class="align-right" src="../../_images/digits_first_image.png" style="width: 166.0px; height: 150.0px;" /></a>
<p>The digits dataset is made of 1797 images, where each one is a 8x8
pixel image representing a hand-written digits</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">digits</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_digits</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(1797, 8, 8)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pylab</span> <span class="kn">as</span> <span class="nn">pl</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pl</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="n">pl</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray_r</span><span class="p">)</span> 
<span class="go">&lt;matplotlib.image.AxesImage object at ...&gt;</span>
</pre></div>
</div>
<p>To use this dataset with the scikit, we transform each 8x8 image in a
feature vector of length 64</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="learning-and-predicting">
<h3><a class="toc-backref" href="#id2">2.9.1.1. Learning and Predicting</a><a class="headerlink" href="#learning-and-predicting" title="Permalink to this headline">¶</a></h3>
<p>Now that we&#8217;ve got some data, we would like to learn from the data and
predict on new one. In <tt class="docutils literal"><span class="pre">scikit-learn</span></tt>, we learn from existing
data by creating an <tt class="docutils literal"><span class="pre">estimator</span></tt> and calling its <tt class="docutils literal"><span class="pre">fit(X,</span> <span class="pre">Y)</span></tt> method.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scikits.learn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">LinearSVC</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span> <span class="c"># learn form the data</span>
</pre></div>
</div>
<p>Once we have learned from the data, we can access the parameters of
the model:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">coef_</span>
<span class="gp">...</span>
</pre></div>
</div>
<p>And it can be used to predict the most likely outcome on unseen data:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span> <span class="mf">5.0</span><span class="p">,</span>  <span class="mf">3.6</span><span class="p">,</span>  <span class="mf">1.3</span><span class="p">,</span>  <span class="mf">0.25</span><span class="p">]])</span>
<span class="go">array([0], dtype=int32)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="supervised-learning">
<h2><a class="toc-backref" href="#id3">2.9.2. Supervised learning</a><a class="headerlink" href="#supervised-learning" title="Permalink to this headline">¶</a></h2>
<div class="section" id="k-nearest-neighbors-classifier">
<h3><a class="toc-backref" href="#id4">2.9.2.1. k-Nearest neighbors classifier</a><a class="headerlink" href="#k-nearest-neighbors-classifier" title="Permalink to this headline">¶</a></h3>
<p>The simplest possible classifier is the nearest neighbor: given a new
observation, take the label of the closest learned observation.</p>
<a class="reference internal image-reference" href="../../_images/iris_knn.png"><img align="right" alt="../../_images/iris_knn.png" class="align-right" src="../../_images/iris_knn.png" style="width: 360.0px; height: 270.0px;" /></a>
<p><strong>KNN (k nearest neighbors) classification example</strong>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="c"># Create and fit a nearest-neighbor classifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scikits.learn</span> <span class="kn">import</span> <span class="n">neighbors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">knn</span> <span class="o">=</span> <span class="n">neighbors</span><span class="o">.</span><span class="n">NeighborsClassifier</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="go">NeighborsClassifier(n_neighbors=5, leaf_size=20, algorithm=&#39;auto&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">]])</span>
<span class="go">array([0])</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title first">Training set and testing set</p>
<p>When experimenting with learning algorithm, it is important not to
test the prediction of an estimator on the data used to fit the
estimator.</p>
</div>
</div>
<div class="section" id="support-vector-machines-svms-for-classification">
<h3><a class="toc-backref" href="#id5">2.9.2.2. Support vector machines (SVMs) for classification</a><a class="headerlink" href="#support-vector-machines-svms-for-classification" title="Permalink to this headline">¶</a></h3>
<div class="section" id="linear-support-vector-machines">
<h4>2.9.2.2.1. Linear Support Vector Machines<a class="headerlink" href="#linear-support-vector-machines" title="Permalink to this headline">¶</a></h4>
<p>SVMs try to build a plane maximizing the margin between the two
classes. It selects a subset of the input, called the support vectors,
which are the observations closest to the separating plane.</p>
<a class="reference internal image-reference" href="../../_images/svm_margin.png"><img align="right" alt="../../_images/svm_margin.png" class="align-right" src="../../_images/svm_margin.png" style="width: 320.0px; height: 240.0px;" /></a>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scikits.learn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">svc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">&#39;linear&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">svc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="go">SVC(kernel=&#39;linear&#39;, C=1.0, probability=False, degree=3, coef0=0.0, tol=0.001,</span>
<span class="go">  shrinking=True, gamma=0.0)</span>
</pre></div>
</div>
<p>There are several support vector machine implementations in
scikit-learn. The most used ones are <tt class="docutils literal"><span class="pre">svm.SVC</span></tt>, <tt class="docutils literal"><span class="pre">svm.NuSVC</span></tt> and <tt class="docutils literal"><span class="pre">svm.LinearSVC</span></tt>.</p>
<div class="green topic">
<p class="topic-title first"><strong>Excercise</strong></p>
<p>Try classifying the digits dataset with <tt class="docutils literal"><span class="pre">svm.SVC</span></tt>. Leave out the
last 10% and test prediction performance on these observations.</p>
</div>
</div>
<div class="section" id="using-kernels">
<h4>2.9.2.2.2. Using kernels<a class="headerlink" href="#using-kernels" title="Permalink to this headline">¶</a></h4>
<p>Classes are not always separable by a hyper-plane, thus it would be
desirable to a build decision function that is not linear but that may
be for instance polynomial or exponential:</p>
<table border="1" class="centered docutils">
<colgroup>
<col width="33%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<tbody valign="top">
<tr><td><strong>Linear kernel</strong></td>
<td><strong>Polynomial kernel</strong></td>
<td><strong>RBF kernel (Radial Basis Function)</strong></td>
</tr>
<tr><td><a class="reference internal" href="../../_images/svm_kernel_linear.png"><img alt="svm_kernel_linear" src="../../_images/svm_kernel_linear.png" style="width: 260.0px; height: 195.0px;" /></a></td>
<td><a class="reference internal" href="../../_images/svm_kernel_poly.png"><img alt="svm_kernel_poly" src="../../_images/svm_kernel_poly.png" style="width: 260.0px; height: 195.0px;" /></a></td>
<td><a class="reference internal" href="../../_images/svm_kernel_rbf.png"><img alt="svm_kernel_rbf" src="../../_images/svm_kernel_rbf.png" style="width: 260.0px; height: 195.0px;" /></a></td>
</tr>
<tr><td><div class="first last highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">svc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">&#39;linear&#39;</span><span class="p">)</span>
</pre></div>
</div>
</td>
<td><div class="first last highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">svc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">&#39;poly&#39;</span><span class="p">,</span>
<span class="gp">... </span>              <span class="n">degree</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c"># degree: polynomial degree</span>
</pre></div>
</div>
</td>
<td><div class="first last highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">svc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">&#39;rbf&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c"># gamma: inverse of size of</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c"># radial kernel</span>
</pre></div>
</div>
</td>
</tr>
</tbody>
</table>
<div class="green topic">
<p class="topic-title first"><strong>Exercise</strong></p>
<p>Which of the kernels noted above has a better prediction
performance on the digits dataset ?</p>
<div class="toctree-wrapper compound">
<ul class="simple">
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="clustering-grouping-observations-together">
<h2><a class="toc-backref" href="#id6">2.9.3. Clustering: grouping observations together</a><a class="headerlink" href="#clustering-grouping-observations-together" title="Permalink to this headline">¶</a></h2>
<p>Given the iris dataset, if we knew that there were 3 types of Iris,
but did not have access to their labels: we could try a <strong>clustering
task</strong>: split the observations into groups called <em>clusters</em>.</p>
<div class="section" id="k-means-clustering">
<h3><a class="toc-backref" href="#id7">2.9.3.1. K-means clustering</a><a class="headerlink" href="#k-means-clustering" title="Permalink to this headline">¶</a></h3>
<p>The simplest clustering algorithm is the k-means.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scikits.learn</span> <span class="kn">import</span> <span class="n">cluster</span><span class="p">,</span> <span class="n">datasets</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k_means</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">KMeans</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k_means</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">)</span> 
<span class="go">KMeans(verbose=0, k=3, max_iter=300, init=&#39;k-means++&#39;,...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">k_means</span><span class="o">.</span><span class="n">labels_</span><span class="p">[::</span><span class="mi">10</span><span class="p">]</span>
<span class="go">[1 1 1 1 1 0 0 0 0 0 2 2 2 2 2]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">[::</span><span class="mi">10</span><span class="p">]</span>
<span class="go">[0 0 0 0 0 1 1 1 1 1 2 2 2 2 2]</span>
</pre></div>
</div>
<table border="1" class="centered docutils">
<colgroup>
<col width="33%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<tbody valign="top">
<tr><td><a class="reference internal" href="../../_images/cluster_iris_truth.png"><img alt="cluster_iris_truth" src="../../_images/cluster_iris_truth.png" style="width: 308.0px; height: 231.0px;" /></a></td>
<td><a class="reference internal" href="../../_images/k_means_iris_3.png"><img alt="cluster_iris_kmeans" src="../../_images/k_means_iris_3.png" style="width: 320.0px; height: 240.0px;" /></a></td>
<td><a class="reference internal" href="../../_images/k_means_iris_8.png"><img alt="k_means_iris_8" src="../../_images/k_means_iris_8.png" style="width: 308.0px; height: 231.0px;" /></a></td>
</tr>
<tr><td><strong>Ground truth</strong></td>
<td><strong>K-means (3 clusters)</strong></td>
<td><strong>K-means (8 clusters)</strong></td>
</tr>
</tbody>
</table>
<div class="topic">
<p class="topic-title first"><strong>Application to Image Compression</strong></p>
<p>Clustering can be seen as a way of choosing a small number of
observations from the information. For instance, this can be used
to posterize an image (conversion of a continuous gradation of
tone to several regions of fewer tones):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">scipy</span> <span class="kn">as</span> <span class="nn">sp</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lena</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">lena</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">lena</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="c"># We need an (n_sample, n_feature) array</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k_means</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">KMeans</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k_means</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">values</span> <span class="o">=</span> <span class="n">k_means</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="n">k_means</span><span class="o">.</span><span class="n">labels_</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lena_compressed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">choose</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">values</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lena_compressed</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">lena</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<table border="1" class="centered docutils">
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<tbody valign="top">
<tr><td><a class="reference internal" href="../../_images/lena1.png"><img alt="lena" src="../../_images/lena1.png" style="width: 256.0px; height: 256.0px;" /></a></td>
<td><a class="reference internal" href="../../_images/lena_compressed.png"><img alt="lena_compressed" src="../../_images/lena_compressed.png" style="width: 256.0px; height: 256.0px;" /></a></td>
</tr>
<tr><td>Raw image</td>
<td>K-means quantization</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="dimension-reduction-with-principal-component-analysis">
<h3><a class="toc-backref" href="#id8">2.9.3.2. Dimension Reduction with Principal Component Analysis</a><a class="headerlink" href="#dimension-reduction-with-principal-component-analysis" title="Permalink to this headline">¶</a></h3>
<p class="centered"><a class="reference internal" href="../../_images/pca_3d_axis.jpg"><img alt="pca_3d_axis" src="../../_images/pca_3d_axis.jpg" style="width: 280.0px; height: 224.0px;" /></a> <a class="reference internal" href="../../_images/pca_3d_aligned.jpg"><img alt="pca_3d_aligned" src="../../_images/pca_3d_aligned.jpg" style="width: 280.0px; height: 224.0px;" /></a></p>
<p>The cloud of points spanned by the observations above is very flat in
one direction, so that one feature can almost be exactly computed
using the 2 other. PCA finds the directions in which the data is not
<em>flat</em></p>
<p>When used to <em>transform</em> data, PCA can reduce the dimensionality of the
data by projecting on a principal subspace.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Depending on your version of scikit-learn PCA will be in module
<tt class="docutils literal"><span class="pre">decomposition</span></tt> or <tt class="docutils literal"><span class="pre">pca</span></tt>.</p>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scikits.learn</span> <span class="kn">import</span> <span class="n">decomposition</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pca</span> <span class="o">=</span> <span class="n">decomposition</span><span class="o">.</span><span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="go">PCA(copy=True, n_components=2, whiten=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<p>Now we can visualize the (transformed) iris dataset!</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pylab</span> <span class="kn">as</span> <span class="nn">pl</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pl</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pl</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="../../_images/pca_iris.png"><div align="center" class="align-center"><img alt="../../_images/pca_iris.png" class="align-center" src="../../_images/pca_iris.png" style="width: 400.0px; height: 300.0px;" /></div>
</a>
<p>PCA is not just useful for visualization of high dimensional
datasets. It can also be used as a preprocessing step to help speed up
supervised methods that are not computationally efficient with high
dimensions.</p>
</div>
</div>
<div class="section" id="putting-it-all-together-face-recognition-with-support-vector-machines">
<h2><a class="toc-backref" href="#id9">2.9.4. Putting it all together : face recognition with Support Vector Machines</a><a class="headerlink" href="#putting-it-all-together-face-recognition-with-support-vector-machines" title="Permalink to this headline">¶</a></h2>
<p>An example showcasing face recognition using Principal Component
Analysis for dimension reduction and Support Vector Machines for
classification.</p>
<a class="reference internal image-reference" href="../../_images/faces.png"><div align="center" class="align-center"><img alt="../../_images/faces.png" class="align-center" src="../../_images/faces.png" style="width: 360.0px; height: 360.0px;" /></div>
</a>
<div class="highlight-python"><div class="highlight"><pre><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Stripped-down version of the face recognition example by Olivier Grisel</span>

<span class="sd">http://scikit-learn.sourceforge.net/dev/auto_examples/applications/face_recognition.html</span>

<span class="sd">## original shape of images: 50, 37</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scikits.learn</span> <span class="kn">import</span> <span class="n">cross_val</span><span class="p">,</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">decomposition</span><span class="p">,</span> <span class="n">svm</span>

<span class="c"># ..</span>
<span class="c"># .. load data ..</span>
<span class="n">lfw_people</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">fetch_lfw_people</span><span class="p">(</span><span class="n">min_faces_per_person</span><span class="o">=</span><span class="mi">70</span><span class="p">,</span> <span class="n">resize</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
<span class="n">faces</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">lfw_people</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="n">lfw_people</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">cross_val</span><span class="o">.</span><span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">lfw_people</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">4</span><span class="p">))</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">faces</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">faces</span><span class="p">[</span><span class="n">test</span><span class="p">]</span>
<span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">lfw_people</span><span class="o">.</span><span class="n">target</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">lfw_people</span><span class="o">.</span><span class="n">target</span><span class="p">[</span><span class="n">test</span><span class="p">]</span>

<span class="c"># ..</span>
<span class="c"># .. dimension reduction ..</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">decomposition</span><span class="o">.</span><span class="n">RandomizedPCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">whiten</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_train_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c"># ..</span>
<span class="c"># .. classification ..</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_pca</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c"># ..</span>
<span class="c"># .. predict on new images ..</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">):</span>
    <span class="k">print</span> <span class="n">lfw_people</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_pca</span><span class="p">[</span><span class="n">i</span><span class="p">])[</span><span class="mi">0</span><span class="p">]]</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">37</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">pl</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray</span><span class="p">)</span>
    <span class="n">_</span> <span class="o">=</span> <span class="nb">raw_input</span><span class="p">()</span>
</pre></div>
</div>
<p>Full code: <a class="reference download internal" href="../../_downloads/faces.py"><tt class="xref download docutils literal"><span class="pre">faces.py</span></tt></a></p>
</div>
</div>


          </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../sympy.html" title="2.8. Sympy : Symbolic Mathematics in Python"
             >previous</a></li>
        <li><a href="../../index.html">Scipy lecture notes</a> &raquo;</li>
          <li><a href="../index.html" >2. Advanced topics</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2011.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.0.
    </div>
  </body>
</html>